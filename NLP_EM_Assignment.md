# NLP_EM算法大作业

---

ZY2103203 黄旭聪 github:

---

## 1. 问题阐述

---

一个袋子中三种硬币的混合比例为：s1, s2与1-s1-s2 (0<=si<=1), 三种硬币掷出正面的概率分别为：p, q, r。 （1）自己指定系数s1, s2, p, q, r，生成N个投掷硬币的结果（由01构成的序列，其中1为正面，0为反面），利用EM算法来对参数进行估计并与预先假定的参数进行比较。 

       该问题为参数估计问题。现实中，模型对应的参数往往是复杂的，混合的。参数的分布往往是不可观的。在简化下，可以认为参数的分布是服从某个理想分布的，即参数的背后还存在隐变量来生成参数。当然，参数的可能不能由单一的分布来表示，故需要不断优化隐变量的分布，使其逼近真实的分布；在估计的隐变量基础上，利用极大似然估计参数。

       EM算法基于此思路，将参数优化过程变为:

1. E步骤:计算期望，利用对隐含变量的估计值，通过最大似然估计得到参数的估计值
2. M步骤：利用参数估计值，根据KL散度，更新隐含变量的估计值

## 2.背景知识

---

- 伯努利分布模型
    
    如果随机变量X只取0和1两个值，并且相应的概率为：
    

$$
Pr(X=1)=p,Pr(X=0)=1-p,0<p<1
$$

      则X的概率分布可以写为：

$$
f(x\mid p)=\begin{cases} p^{x} q^{1-x}  & \text{ if } x=0,1 \\ 0 & \text{ if } x\not{=0,1} \end{cases}
$$

- EM算法具体化

      针对背景，这里隐含变量为s1,s2,1-s1-s2,分别对应选择三个硬币的概率。选中的硬币进行投掷，正反面结果为当前一次试验的结果。其中三个硬币抛掷结果独立，分别依概率p,q,r投掷出正面。

      每一次试验具备完备性，即一定会从三个硬币中选择处一个，因此添加约束：

$$
0<s_{1} +s_{2}<1 
$$

      已知试验结果x，估计对应的参数(p,q,r)∈θ，需要用到极大似然法。极大似然估计需要对于似然函数求导，但是这个求导过程很复杂。故引入隐含变量z_{i}，使得求导可以继续，极大似然函数如下：

$$
L(\theta )=log(\prod_{i=1}^{n}p(x_{i}\mid \theta  ) )=log(\prod_{i=1}^{n}p(x_{i},z_{i}\mid \theta  ) )
$$

       通过理想分布近似逼近隐含变量的真实分布函数Q(z)，也对应EM算法中的M步。根据隐含变量求解极大似然值，得到参数的最优估计，也对应EM算法中的E步。极大似然求解问题就变为了：

$$
L(\theta )=\sum_{i=1}^{n} log\sum_{z_{i} }^{}Q(z_{i})\frac{p(x_{i},z_{i}\mid \theta )}{Q(z_{i})} \ge\sum_{i=1}^{n}\sum_{z_{i} }{Q(z_{i})}log\frac{p(x_{i},z_{i}\mid \theta )}{Q(z_{i})} 
$$

其中z_{i}为隐含变量，选中1硬币对应p(z_{i}=1\mid \theta  ) =s1,选中2硬币对应p(z_{i}=2\mid \theta  ) =s2，选中3硬币对应p(z_{i}=3\mid \theta  ) =s3，与p,q,r共同组成参数θ。不断通过更新隐含变量的分布估计抬高下界，从而得到更大的似然值，更准确的估计θ。如上述过程，很容易陷入到局部最优解，也是EM算法的局限所在。

       针对背景，利用参数θ估计隐变量服从的分布，usi^j代表来自于第j次实验来源于第i个硬币的可能性，则有：

$$
u_{si}^{j}=\frac{\pi_{si}p^{xi}(1-p)^{1-xi}  }{\sum_{i} \pi_{si}p^{xi}(1-p)^{1-xi}} 
$$

之后，利用隐变量的分布估计θ，具体为：

$$
\begin{array}{l}   \pi_{s1}=\sum_{j=1}^{N} u_{s1}^{j} \\   \pi_{s2}=\sum_{j=1}^{N} u_{s2}^{j} \\   p=\frac{\sum_{j=1}^{N} u_{s1}^{j}x^j }{\sum_{j=1}^{N} u_{s1}^{j}}  \\   q=\frac{\sum_{j=1}^{N} u_{s2}^{j}x^j }{\sum_{j=1}^{N} u_{s2}^{j}} \\   r=\frac{\sum_{j=1}^{N} (1-u_{s1}^{j}-u_{s2}^{j})x^j }{\sum_{j=1}^{N} (1-u_{s1}^{j}-u_{s2}^{j})} \end{array} 
$$

重复上述的E、M过程直至参数收敛。

## 3.实验部分

---

- 模拟数据的生成

       给定隐含变量的参数，即选择第i个硬币的概率，并给定每个硬币投掷正面的概率，统计实验得到的一些列正反面结果。为了比较数据量对于估计准确性的影响，故选取1200，3600，6000，7400.

| 1.初始化θ,包括(s1,s2,p,q,r)以及点数num |
| --- |
| 2.for i in range(len(num))
        随机一个在（0，1）的随机数θ，if θ>p 产生硬币1的投掷结果
                                                              Else if θ>p+q 产生硬币2的投掷结果
                                                                     Else 产生硬币3的投掷结果 |
| 3.OUTPUT:num个产生的投掷结果 |
- EM算法的实现

| 1.INPUT:数据x=(x1,x2,...,xn), 最大迭代次数max_iter，参数初始值θ |
| --- |
| 2.for i in range(len(num)) |
| 3.E步：计算隐含变量服从分布的条件概率期望 |
|                 ⁍ |
| 4.M步：极大化似然函数，得到参数 |
|                 ⁍ |
| 5.if 参数收敛 break |
| 6.OUTPUT |

## 4.实验结果及分析

1. 不同初值的影响

| 参数真实值
（s1,s2,p,q,r） | 样本数 | 隐含变量s1,s2初值 | p,q,r初值 | 隐含变量s1,s2估计值 | p,q,r估计值 | 迭代次数 |
| --- | --- | --- | --- | --- | --- | --- |
| （0.2，0.5，0.3，0.6，0.9） | 12000 | 0.1，0.7 | 0.2，0.8，0.3 | 0.098，0.706 | 0.209，0.809，0.312 | 2 |
| （0.2，0.5，0.3，0.6，0.9） | 12000 | 0.2，0.5 | 0.2，0.8，0.3 | 0.167，0.568 | 0.295，0.870，0.417 | 2 |
| （0.2，0.5，0.3，0.6，0.9） | 12000 | 0.1，0.7 | 0.3，0.6，0.9 | 0.097，0.698 | 0.322，0.624，0.909 | 2 |
| （0.2，0.5，0.3，0.6，0.9） | 12000 | 0.3，0.4 | 0.4，0.5，0.8 | 0.282，0.391 | 0.497，0.597，0.856 | 2 |

样本数选用12000，通过控制变量改变初值，可以得到以下结论：

- 第一组数据给定的偏移量较大，因此不能找到较为接近真实值的估计，主要有两点原因：1.python自带的随机数生成器为伪随机数，故其生成的概率与设定的参数p,q,r就存在差异

      2.使用的EM算法可以看为forward KL，即其估计的隐含变量分布更平缓，而实际的隐含变量为两个有间隔的离散值，故在估计中会平滑处理。

- 控制变量下，可以看出初值对于收敛结果影响很大，主要原因是EM算法容易陷入局部最优解，因此在应用时通常采用ensemble方法。
- 当初值给定在离真实值较近的范围内，其参数估计结果最为准确，更加说明了初值对于EM算法的影响。

2.不同样本量的影响

| 参数真实值
（s1,s2,p,q,r） | 样本数 | 隐含变量s1,s2初值 | p,q,r初值 | 隐含变量s1,s2估计值 | p,q,r估计值 | 迭代次数 |
| --- | --- | --- | --- | --- | --- | --- |
| （0.2，0.5，0.3，0.6，0.9） | 12000 | 0.3，0.4 | 0.4，0.5，0.8 | 0.282，0.391 | 0.497，0.597，0.856 | 2 |
| （0.2，0.5，0.3，0.6，0.9） | 48000 | 0.3，0.4 | 0.4，0.5，0.8 | 0.290，0.395 | 0.453，0.554，0.833 | 2 |
| （0.2，0.5，0.3，0.6，0.9） | 96000 | 0.3，0.4 | 0.4，0.5，0.8 | 0.290，0.395 | 0.453，0.554，0.832 | 2 |

选用1中的最优数据，作为的2的对比集。分别对比不同样本长度，可以发现长度越长，估计准确度就越高。

## 5.总结

通过本次的大作业，我复习了EM算法的相关知识。从理解EM算法数学原理、手推EM算法相关公式再到实际的编程过程，让我对于相关知识的理解更加深刻。特别是相关数学原理的推理，对于我夯实基础尤为关键。对于我在本专业中应用其他基于概率的方法，如基于随机过程的预测，粒子滤波等大有益处。
总的来说，这次大作业更像是一个项目，从动机到分析到实现，很好的锻炼了我的科研能力和解决问题的实际能力。唯一的遗憾就是没有足够的时间优化EM算法，例如加入半监督学习的思想，解决数据量不够带来的误差问题。通过大作业也发现了我还存在的不足，也是一种持续的激励，将鼓舞我更加努力认真的对待我的专业，我的兴趣！